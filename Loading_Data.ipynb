{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4866e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn\n",
    "import pandas as pd \n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm, trange\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc6d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv('./FracAtlas/dataset.csv')\n",
    "fractured_df = label_df[label_df['fractured'] == 1]\n",
    "unfractured_df = label_df[label_df['fractured'] == 0]\n",
    "\n",
    "train_frac_df, val_frac_df = train_test_split(fractured_df, test_size=0.2)\n",
    "train_unfrac_df, val_unfrac_df = train_test_split(unfractured_df, test_size=0.2)\n",
    "\n",
    "train_df = pd.concat((train_frac_df, train_unfrac_df))\n",
    "val_df = pd.concat((val_frac_df, val_unfrac_df))\n",
    "\n",
    "val_df.reset_index(inplace = True)\n",
    "train_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efdd4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotation_df, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = annotation_df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.loc[idx, 'image_id'])\n",
    "#         print(img_path)\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = torch.tensor(image, dtype= torch.float32).unsqueeze(0)\n",
    "#         image = torch.tensor(image).unsqueeze(0)\n",
    "        label = self.img_labels.loc[idx, 'fractured']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b523582",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "#     transforms.RandomRotation(30),  # Randomly rotate the image\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5),  # Adjust brightness and contrast\n",
    "    transforms.Normalize(mean=[0.406], std=[0.225])\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = CustomImageDataset(train_df, './FracAtlas/images/Compiled/', transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=24, shuffle=True)\n",
    "\n",
    "val_dataset = CustomImageDataset(val_df, './FracAtlas/images/Compiled/', transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=24, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abfc378",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([24, 1, 256, 256])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ceae344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.encoding_size = config['encoding_size']\n",
    "        self.img_dim = config['img_dim']\n",
    "        self.dropout = config['dropout']\n",
    "        self.patch_size = config['patch_size']\n",
    "        assert self.img_dim % self.patch_size == 0, (\"Image is not compatible with number of patches\")\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, self.encoding_size))\n",
    "        self.n_patches = (self.img_dim//self.patch_size)**2\n",
    "        self.encoder = nn.Conv2d(1, self.encoding_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, self.n_patches + 1, self.encoding_size))\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        cls_token = self.cls_token.expand((x.shape[0], -1, -1))\n",
    "        out = out.flatten(-2).transpose(-1, -2)\n",
    "        out = torch.cat((cls_token, out), dim = 1)\n",
    "        print(out.shape, self.pos_encoder.shape)\n",
    "        out = out + self.pos_encoder\n",
    "        return self.dropout_layer(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee50f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.d_model = config['encoding_size']           # Total dimension of the model\n",
    "        self.num_heads = config['num_heads']       # Number of attention heads\n",
    "        assert self.d_model % self.num_heads == 0, 'd_model must be divisible by num_heads'\n",
    "        self.d_k = self.d_model // self.num_heads  # Dimnsion of each head. We assume d_v = d_k\n",
    "\n",
    "        # Linear transformations for queries, keys, and values\n",
    "        self.W_q = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_k = nn.Linear(self.d_model, self.d_model)\n",
    "        self.W_v = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "        # Final linear layer to project the concatenated heads' outputs back to d_model dimensions\n",
    "        self.W_o = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attn_weights = torch.softmax(scores / math.sqrt(self.d_k), dim = -1)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q_proj = self.W_q(x)\n",
    "        K_proj = self.W_k(x)\n",
    "        V_proj = self.W_v(x)\n",
    "\n",
    "        Q_proj_split = self.split_heads(Q_proj)\n",
    "        K_proj_split = self.split_heads(K_proj)\n",
    "        V_proj_split = self.split_heads(V_proj)\n",
    "\n",
    "        attention_scores = self.scaled_dot_product_attention(Q_proj_split, K_proj_split, V_proj_split)\n",
    "\n",
    "        output = self.combine_heads(attention_scores)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2743a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoding_size = config['encoding_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "        self.dense_1 = nn.Linear(self.encoding_size, self.intermediate_size)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dense_2 = nn.Linear(self.intermediate_size, self.encoding_size)\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c43e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.encoding_size = config['encoding_size']\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(self.encoding_size)\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(self.encoding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output = self.attention(self.layernorm_1(x))\n",
    "        x = x + attention_output\n",
    "        mlp_output = self.mlp(self.layernorm_2(x))\n",
    "        x = x + mlp_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e250ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a838d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTForClassfication(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.encoding_size = config[\"encoding_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.embedding = PatchEncoder(config)\n",
    "        self.encoder = Encoder(config)\n",
    "        self.classifier = nn.Linear(self.encoding_size, self.num_classes)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        embedding_output = self.embedding(x)\n",
    "        encoder_output = self.encoder(embedding_output)\n",
    "        logits = self.classifier(encoder_output[:, 0])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c35784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                  | 0/5 [00:00<?, ?it/s]\n",
      "Epoch 1 in training:   0%|                                                                     | 0/137 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   1%|▍                                                            | 1/137 [00:03<07:09,  3.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   1%|▉                                                            | 2/137 [00:04<04:14,  1.89s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   2%|█▎                                                           | 3/137 [00:06<04:40,  2.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   3%|█▊                                                           | 4/137 [00:08<04:50,  2.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   4%|██▏                                                          | 5/137 [00:11<04:55,  2.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   4%|██▋                                                          | 6/137 [00:13<04:57,  2.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   5%|███                                                          | 7/137 [00:15<04:57,  2.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   6%|███▌                                                         | 8/137 [00:18<04:57,  2.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   7%|████                                                         | 9/137 [00:20<04:56,  2.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([2, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   7%|████▍                                                       | 10/137 [00:36<13:37,  6.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10: Validation Loss: 0.57, Validation Accuracy: 0.80\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   8%|████▊                                                       | 11/137 [00:37<10:05,  4.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   9%|█████▎                                                      | 12/137 [00:39<08:26,  4.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:   9%|█████▋                                                      | 13/137 [00:41<07:18,  3.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:  10%|██████▏                                                     | 14/137 [00:44<06:29,  3.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:  11%|██████▌                                                     | 15/137 [00:46<05:56,  2.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:  12%|███████                                                     | 16/137 [00:48<05:31,  2.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:  12%|███████▍                                                    | 17/137 [00:51<05:14,  2.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:  13%|███████▉                                                    | 18/137 [00:53<05:01,  2.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:  14%|████████▎                                                   | 19/137 [00:55<04:51,  2.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:  15%|████████▊                                                   | 20/137 [01:11<12:12,  6.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1025, 128]) torch.Size([1, 1025, 128])\n",
      "Iteration 20: Validation Loss: 0.46, Validation Accuracy: 0.82\n",
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:  15%|█████████▏                                                  | 21/137 [01:11<09:00,  4.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:  16%|█████████▋                                                  | 22/137 [01:14<07:35,  3.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 1025, 128]) torch.Size([1, 1025, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 in training:  17%|██████████                                                  | 23/137 [01:16<06:35,  3.47s/it]\u001b[A\n",
      "Training:   0%|                                                                                  | 0/5 [01:18<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Training Step\u001b[39;00m\n\u001b[0;32m     39\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 40\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mcuda()  \u001b[38;5;66;03m# Assuming you're using a GPU\u001b[39;00m\n\u001b[0;32m     41\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m vit(x)\n\u001b[0;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# img = torch.rand((23, 1, 35, 35))\n",
    "# config = {\n",
    "#     'encoding_size': 1024,\n",
    "#     'patch_size': 10,\n",
    "#     'img_dim': 400,\n",
    "#     'dropout': 0.1,\n",
    "#     'num_heads': 6,\n",
    "#     'num_hidden_layers': 3,\n",
    "#     'intermediate_size': 2048,\n",
    "#     'num_classes': 2\n",
    "# }\n",
    "\n",
    "config = {\n",
    "    \"encoding_size\": 128,  # Change this according to your ViT model\n",
    "    \"num_classes\": 2,     # MNIST has 10 output classes (digits 0-9)\n",
    "    \"num_heads\": 8,\n",
    "    \"img_dim\": 256,         # Resize the MNIST images to 32x32\n",
    "    \"patch_size\": 8,       # Assuming 32x32 images, patch size = 4 means 8x8 patches\n",
    "    \"dropout\": 0.1,\n",
    "    'num_hidden_layers': 3,\n",
    "    'intermediate_size': 4*128,\n",
    "}\n",
    "# Model (replace `ViTForClassification` with your actual ViT class name)\n",
    "vit = ViTForClassfication(config)\n",
    "vit = vit.cuda()  \n",
    "\n",
    "optimizer = Adam(vit.parameters(), lr=0.0005)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in trange(5, desc=\"Training\"):\n",
    "    train_loss = 0.0\n",
    "    iteration = 0  # To track the number of iterations\n",
    "\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "        iteration += 1\n",
    "        \n",
    "        # Training Step\n",
    "        x, y = batch\n",
    "        x, y = x.cuda(), y.cuda()  # Assuming you're using a GPU\n",
    "        y_hat = vit(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        train_loss += loss.detach().cpu().item() / len(train_dataloader)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step after every 10 iterations\n",
    "        if iteration % 10 == 0:\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            with torch.no_grad():  # Disable gradient computation\n",
    "                for val_batch in val_dataloader:\n",
    "                    val_x, val_y = val_batch\n",
    "                    val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "\n",
    "                    val_y_hat = vit(val_x)\n",
    "                    val_loss += criterion(val_y_hat, val_y).item() / len(val_dataloader)\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    _, predicted = torch.max(val_y_hat, 1)\n",
    "                    correct += (predicted == val_y).sum().item()\n",
    "                    total += val_y.size(0)\n",
    "\n",
    "            val_accuracy = correct / total\n",
    "\n",
    "            print(f\"Iteration {iteration}: Validation Loss: {val_loss:.2f}, Validation Accuracy: {val_accuracy:.2f}\")\n",
    "            model.train()  # Switch back to training mode\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "fb7ab8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim import Adam\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "# from torchvision import datasets, transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "# from tqdm import trange, tqdm\n",
    "\n",
    "# # Configuration for the ViT model (you can adjust these as per your model design)\n",
    "# config = {\n",
    "#     \"encoding_size\": 128,  # Change this according to your ViT model\n",
    "#     \"num_classes\": 10,     # MNIST has 10 output classes (digits 0-9)\n",
    "#     \"num_heads\": 8,\n",
    "#     \"img_dim\": 32,         # Resize the MNIST images to 32x32\n",
    "#     \"patch_size\": 4,       # Assuming 32x32 images, patch size = 4 means 8x8 patches\n",
    "#     \"dropout\": 0.1,\n",
    "#     'num_hidden_layers': 3,\n",
    "#     'intermediate_size': 4*128,\n",
    "# }\n",
    "\n",
    "# # Model (replace `ViTForClassification` with your actual ViT class name)\n",
    "# model = ViTForClassfication(config)\n",
    "# model = model.cuda()  # Move the model to the GPU\n",
    "\n",
    "# # Preprocessing and loading the MNIST dataset\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((32, 32)),  # Resize the 28x28 images to 32x32\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] range\n",
    "# ])\n",
    "\n",
    "# # Load the MNIST dataset\n",
    "# train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "# val_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# # Create DataLoader for training and validation\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# # Optimizer and loss function\n",
    "# optimizer = Adam(model.parameters(), lr=0.001)\n",
    "# criterion = CrossEntropyLoss()\n",
    "\n",
    "# # Training loop\n",
    "# N_EPOCHS = 5\n",
    "# for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "#     train_loss = 0.0\n",
    "#     iteration = 0  # To track the number of iterations\n",
    "\n",
    "#     for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "#         iteration += 1\n",
    "\n",
    "#         # Training Step\n",
    "#         x, y = batch\n",
    "#         x, y = x.cuda(), y.cuda()  # Assuming you're using a GPU\n",
    "#         y_hat = model(x)\n",
    "#         loss = criterion(y_hat, y)\n",
    "\n",
    "#         train_loss += loss.detach().cpu().item() / len(train_dataloader)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Validation step after every 10 iterations\n",
    "#         if iteration % 10 == 0:\n",
    "#             val_loss = 0.0\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "\n",
    "#             model.eval()  # Set the model to evaluation mode\n",
    "#             with torch.no_grad():  # Disable gradient computation\n",
    "#                 for val_batch in val_dataloader:\n",
    "#                     val_x, val_y = val_batch\n",
    "#                     val_x, val_y = val_x.cuda(), val_y.cuda()\n",
    "\n",
    "#                     val_y_hat = model(val_x)\n",
    "#                     val_loss += criterion(val_y_hat, val_y).item() / len(val_dataloader)\n",
    "\n",
    "#                     # Calculate accuracy\n",
    "#                     _, predicted = torch.max(val_y_hat, 1)\n",
    "#                     correct += (predicted == val_y).sum().item()\n",
    "#                     total += val_y.size(0)\n",
    "\n",
    "#             val_accuracy = correct / total\n",
    "\n",
    "#             print(f\"Iteration {iteration}: Validation Loss: {val_loss:.2f}, Validation Accuracy: {val_accuracy:.2f}\")\n",
    "#             model.train()  # Switch back to training mode\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06783e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
